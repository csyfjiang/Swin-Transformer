# ğŸš€ Alzheimer MMoE è®­ç»ƒæ¢å¤æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µä¸­æ–­åå¦‚ä½•æ­£ç¡®æ¢å¤è®­ç»ƒã€‚

## ğŸ“‹ ç›®å½•
- [1. é¢„è®­ç»ƒé˜¶æ®µä¸­æ–­æ¢å¤](#1-é¢„è®­ç»ƒé˜¶æ®µä¸­æ–­æ¢å¤)
- [2. å¾®è°ƒé˜¶æ®µä¸­æ–­æ¢å¤](#2-å¾®è°ƒé˜¶æ®µä¸­æ–­æ¢å¤)
- [3. checkpointæ–‡ä»¶ç»“æ„](#3-checkpointæ–‡ä»¶ç»“æ„)
- [4. å¸¸è§é—®é¢˜ä¸è§£å†³](#4-å¸¸è§é—®é¢˜ä¸è§£å†³)
- [5. æœ€ä½³å®è·µå»ºè®®](#5-æœ€ä½³å®è·µå»ºè®®)

---

## 1. é¢„è®­ç»ƒé˜¶æ®µä¸­æ–­æ¢å¤

### ğŸ¯ åœºæ™¯æè¿°
åœ¨**SimMIMé¢„è®­ç»ƒé˜¶æ®µ**ï¼ˆepoch < `pretrain_epochs`ï¼‰è®­ç»ƒä¸­æ–­ï¼Œéœ€è¦ä»æ–­ç‚¹ç»§ç»­é¢„è®­ç»ƒã€‚

### ğŸ’¾ checkpointæ–‡ä»¶ä½ç½®
è®­ç»ƒä¸­æ–­æ—¶ä¼šè‡ªåŠ¨ä¿å­˜ä»¥ä¸‹æ–‡ä»¶ï¼š
```
checkpoints/
â”œâ”€â”€ swin_admoe_20250701_143022/
â”‚   â”œâ”€â”€ checkpoint_epoch_25.pth          # å®šæœŸcheckpoint
â”‚   â”œâ”€â”€ pretrain_checkpoint.pth          # æ—©åœä¿å­˜çš„æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹
â”‚   â””â”€â”€ training.log
```

### ğŸ”§ æ¢å¤ä»£ç ç¤ºä¾‹

#### æ–¹æ³•1: ä»å®šæœŸcheckpointæ¢å¤
```python
import torch
from trainer import trainer_alzheimer_mmoe
from models import SwinTransformerV2_AlzheimerMMoE

# åˆ›å»ºå‚æ•°å¯¹è±¡
args = Args()
args.seed = 42
args.max_epochs = 100
args.pretrain_epochs = 50  # ç¡®ä¿ä¸åŸè®­ç»ƒä¸€è‡´
args.eval_interval = 10
args.save_interval = 20

# ğŸ”¥ å…³é”®ï¼šè®¾ç½®resumeè·¯å¾„
args.resume = "checkpoints/swin_admoe_20250701_143022/checkpoint_epoch_25.pth"

# å…¶ä»–è®­ç»ƒå‚æ•°
args.base_lr = 1e-4
args.weight_decay = 1e-4
args.mask_ratio = 0.6
args.patience = 10

# åˆ›å»ºæ¨¡å‹
model = SwinTransformerV2_AlzheimerMMoE(
    img_size=256,
    patch_size=4,
    embed_dim=96,
    depths=[2, 2, 6, 2],
    num_heads=[3, 6, 12, 24],
    num_classes_diagnosis=3,
    num_classes_change=3,
    use_clinical_prior=True,
    # ... å…¶ä»–å‚æ•°
)

# ğŸš€ æ¢å¤è®­ç»ƒ
trainer_alzheimer_mmoe(args, model, "checkpoints/resume_pretrain")
```

#### æ–¹æ³•2: ä»æ—©åœcheckpointæ¢å¤
```python
args.resume = "checkpoints/swin_admoe_20250701_143022/pretrain_checkpoint.pth"
```

### ğŸ“Š é¢„æœŸæ—¥å¿—è¾“å‡º
```
[14:30:22.123] Resuming training from checkpoint...
[14:30:22.456] Loading pretrained weights from: checkpoints/.../checkpoint_epoch_25.pth
[14:30:22.789] Loading from epoch 25, phase: Pretrain (SimMIM)
[14:30:23.012] Original checkpoint contains 142 parameters
[14:30:23.234] Found 8 decoder parameters
[14:30:23.456] Parameter matching analysis:
[14:30:23.678]   - Matched parameters: 142
[14:30:23.890]   - Missing parameters: 0
[14:30:24.012] Resumed from epoch 26
[14:30:24.234] Optimizer state loaded from checkpoint
[14:30:24.456] Scheduler state loaded from checkpoint

==================================================
MODEL COMPONENTS ANALYSIS - INITIAL
==================================================
  decoder             :  2,362,368 params âœ“ Active
  head_diagnosis      :      2,307 params âœ“ Active
  # ... å…¶ä»–ç»„ä»¶

Training plan:
- Pretraining epochs: 50 (SimMIM reconstruction)
- Finetuning epochs: 50 (Classification)
- Starting from epoch: 26
```

---

## 2. å¾®è°ƒé˜¶æ®µä¸­æ–­æ¢å¤

### ğŸ¯ åœºæ™¯æè¿°
åœ¨**åˆ†ç±»å¾®è°ƒé˜¶æ®µ**ï¼ˆepoch >= `pretrain_epochs`ï¼‰è®­ç»ƒä¸­æ–­ï¼Œéœ€è¦ä»æ–­ç‚¹ç»§ç»­å¾®è°ƒã€‚

### ğŸ’¾ checkpointæ–‡ä»¶ä½ç½®
```
checkpoints/
â”œâ”€â”€ swin_admoe_20250701_143022/
â”‚   â”œâ”€â”€ checkpoint_epoch_75.pth          # å®šæœŸcheckpointï¼ˆå¾®è°ƒé˜¶æ®µï¼‰
â”‚   â”œâ”€â”€ finetune_checkpoint.pth          # æ—©åœä¿å­˜çš„æœ€ä½³å¾®è°ƒæ¨¡å‹
â”‚   â”œâ”€â”€ best_model.pth                   # éªŒè¯é›†æœ€ä½³æ¨¡å‹
â”‚   â””â”€â”€ final_model.pth                  # è®­ç»ƒå®Œæˆçš„æœ€ç»ˆæ¨¡å‹
```

### ğŸ”§ æ¢å¤ä»£ç ç¤ºä¾‹

#### æ–¹æ³•1: ä»å®šæœŸcheckpointæ¢å¤
```python
import torch
from trainer import trainer_alzheimer_mmoe
from models import SwinTransformerV2_AlzheimerMMoE

# åˆ›å»ºå‚æ•°å¯¹è±¡
args = Args()
args.seed = 42
args.max_epochs = 100
args.pretrain_epochs = 50  # ç¡®ä¿ä¸åŸè®­ç»ƒä¸€è‡´
args.eval_interval = 10
args.save_interval = 20

# ğŸ”¥ å…³é”®ï¼šè®¾ç½®resumeè·¯å¾„ï¼ˆå¾®è°ƒé˜¶æ®µçš„checkpointï¼‰
args.resume = "checkpoints/swin_admoe_20250701_143022/checkpoint_epoch_75.pth"

# ä»»åŠ¡æƒé‡
args.weight_diagnosis = 1.0
args.weight_change = 1.0
args.label_smoothing = 0.1

# åˆ›å»ºæ¨¡å‹ï¼ˆæ³¨æ„ï¼šdecoderä¼šåœ¨åˆ‡æ¢æ—¶è‡ªåŠ¨ç§»é™¤ï¼‰
model = SwinTransformerV2_AlzheimerMMoE(
    img_size=256,
    patch_size=4,
    embed_dim=96,
    depths=[2, 2, 6, 2],
    num_heads=[3, 6, 12, 24],
    num_classes_diagnosis=3,
    num_classes_change=3,
    use_clinical_prior=True,
    # ... å…¶ä»–å‚æ•°
)

# ğŸš€ æ¢å¤è®­ç»ƒ
trainer_alzheimer_mmoe(args, model, "checkpoints/resume_finetune")
```

#### æ–¹æ³•2: ä»æœ€ä½³æ¨¡å‹æ¢å¤
```python
# å¦‚æœæƒ³ä»éªŒè¯é›†æœ€ä½³æ¨¡å‹ç»§ç»­è®­ç»ƒ
args.resume = "checkpoints/swin_admoe_20250701_143022/best_model.pth"
```

### ğŸ“Š é¢„æœŸæ—¥å¿—è¾“å‡º
```
[16:45:12.123] Resuming training from checkpoint...
[16:45:12.456] Loading pretrained weights from: checkpoints/.../checkpoint_epoch_75.pth
[16:45:12.789] Loading from epoch 75, phase: Finetune (Classification)
[16:45:13.012] Original checkpoint contains 134 parameters
[16:45:13.234] Found 0 decoder parameters  # â† æ³¨æ„ï¼šå¾®è°ƒcheckpointå·²æ— decoder
[16:45:13.456] Parameter matching analysis:
[16:45:13.678]   - Matched parameters: 134
[16:45:13.890]   - Missing parameters: 0
[16:45:14.012] Resumed from epoch 76
[16:45:14.234] Optimizer state loaded from checkpoint
[16:45:14.456] Scheduler state loaded from checkpoint

==================================================
MODEL COMPONENTS ANALYSIS - INITIAL  
==================================================
  clinical_encoder    :    233,216 params âœ“ Active
  clinical_fusion     :  1,772,546 params âœ“ Active
  head_diagnosis      :      2,307 params âœ“ Active
  head_change         :      2,307 params âœ“ Active
  # æ³¨æ„ï¼šæ²¡æœ‰decoderç»„ä»¶

Training plan:
- Pretraining epochs: 50 (SimMIM reconstruction)  
- Finetuning epochs: 50 (Classification)
- Starting from epoch: 76  # â† ç›´æ¥è¿›å…¥å¾®è°ƒé˜¶æ®µ
```

---

## 3. checkpointæ–‡ä»¶ç»“æ„

### ğŸ—‚ï¸ å®Œæ•´checkpointå†…å®¹
```python
checkpoint = {
    'epoch': 75,                          # å½“å‰epoch
    'model_state_dict': state_dict,       # æ¨¡å‹æƒé‡ï¼ˆå·²è¿‡æ»¤decoderï¼‰
    'optimizer_state_dict': opt_state,    # ä¼˜åŒ–å™¨çŠ¶æ€  
    'scheduler_state_dict': sched_state,  # å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
    'best_val_acc': 0.8234,              # æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    'is_pretrain': False,                 # å½“å‰æ˜¯å¦ä¸ºé¢„è®­ç»ƒé˜¶æ®µ
    'phase': 'Finetune (Classification)', # å½“å‰è®­ç»ƒé˜¶æ®µæè¿°
    'decoder_removed': True               # decoderæ˜¯å¦å·²ç§»é™¤
}
```

### ğŸ” å¦‚ä½•æ£€æŸ¥checkpointä¿¡æ¯
```python
import torch

# åŠ è½½å¹¶æ£€æŸ¥checkpoint
checkpoint = torch.load("checkpoint_epoch_75.pth", map_location='cpu')

print(f"Epoch: {checkpoint['epoch']}")
print(f"Phase: {checkpoint['phase']}")  
print(f"Best accuracy: {checkpoint['best_val_acc']:.4f}")
print(f"Decoder removed: {checkpoint.get('decoder_removed', False)}")
print(f"Model parameters: {len(checkpoint['model_state_dict'])}")

# æ£€æŸ¥æ˜¯å¦åŒ…å«decoderæƒé‡
decoder_keys = [k for k in checkpoint['model_state_dict'].keys() 
                if k.startswith('decoder.')]
print(f"Decoder parameters: {len(decoder_keys)}")
```

---

## 4. å¸¸è§é—®é¢˜ä¸è§£å†³

### âŒ é—®é¢˜1: æƒé‡ä¸åŒ¹é…
```
RuntimeError: Error(s) in loading state_dict for SwinTransformerV2_AlzheimerMMoE:
Missing key(s) in state_dict: "decoder.0.weight", "decoder.0.bias"
```

**ğŸ”§ è§£å†³æ–¹æ¡ˆ:**
```python
# æ£€æŸ¥checkpointæ˜¯å¦æ¥è‡ªå¾®è°ƒé˜¶æ®µ
checkpoint = torch.load(resume_path, map_location='cpu')
if checkpoint.get('decoder_removed', False):
    print("âœ“ Loading from finetune checkpoint (no decoder)")
else:
    print("âš ï¸ Loading from pretrain checkpoint (has decoder)")
```

### âŒ é—®é¢˜2: epochæ•°é‡ä¸åŒ¹é…
```
# å‡è®¾åŸè®­ç»ƒï¼špretrain_epochs=50, max_epochs=100
# ä½†resumeæ—¶è®¾ç½®ï¼špretrain_epochs=40, max_epochs=90
```

**ğŸ”§ è§£å†³æ–¹æ¡ˆ:**
```python
# ç¡®ä¿è®­ç»ƒå‚æ•°ä¸åŸå§‹è®­ç»ƒä¸€è‡´
args.pretrain_epochs = 50  # å¿…é¡»ä¸åŸè®­ç»ƒç›¸åŒ
args.max_epochs = 100      # å¿…é¡»ä¸åŸè®­ç»ƒç›¸åŒ
```

### âŒ é—®é¢˜3: å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å¼‚å¸¸
```
Warning: scheduler state loaded but step count mismatch
```

**ğŸ”§ è§£å†³æ–¹æ¡ˆ:**
```python
# æ–¹æ³•1: é‡æ–°åˆ›å»ºschedulerï¼ˆä¸¢å¤±å†å²çŠ¶æ€ï¼‰
# åœ¨trainerä¸­ä¼šè‡ªåŠ¨å¤„ç†

# æ–¹æ³•2: æ£€æŸ¥checkpointå®Œæ•´æ€§
checkpoint = torch.load(resume_path)
if 'scheduler_state_dict' in checkpoint:
    print("âœ“ Scheduler state available")
else:
    print("âš ï¸ Scheduler state missing, will recreate")
```

---

## 5. æœ€ä½³å®è·µå»ºè®®

### ğŸ¯ è®­ç»ƒç­–ç•¥
1. **å®šæœŸä¿å­˜**: è®¾ç½®åˆé€‚çš„ `save_interval`ï¼ˆå»ºè®®10-20 epochsï¼‰
2. **æ—©åœä¿æŠ¤**: å¯ç”¨ `patience` é¿å…è¿‡æ‹Ÿåˆ
3. **å¤‡ä»½é‡è¦checkpoint**: æ‰‹åŠ¨å¤‡ä»½å…³é”®é˜¶æ®µçš„æƒé‡

### ğŸ“ æ–‡ä»¶ç®¡ç†
```bash
# æ¨èçš„checkpointç›®å½•ç»“æ„
checkpoints/
â”œâ”€â”€ experiment_name_timestamp/
â”‚   â”œâ”€â”€ checkpoint_epoch_*.pth     # å®šæœŸcheckpoint
â”‚   â”œâ”€â”€ pretrain_checkpoint.pth    # é¢„è®­ç»ƒæœ€ä½³
â”‚   â”œâ”€â”€ finetune_checkpoint.pth    # å¾®è°ƒæœ€ä½³  
â”‚   â”œâ”€â”€ best_model.pth             # å…¨å±€æœ€ä½³
â”‚   â”œâ”€â”€ final_model.pth            # æœ€ç»ˆæ¨¡å‹
â”‚   â”œâ”€â”€ training.log               # è®­ç»ƒæ—¥å¿—
â”‚   â””â”€â”€ config.yaml                # é…ç½®å¤‡ä»½
```

### ğŸ”§ ä»£ç æ¨¡æ¿
```python
def create_resume_args(original_checkpoint_dir, resume_epoch=None):
    """åˆ›å»ºæ¢å¤è®­ç»ƒçš„å‚æ•°"""
    args = Args()
    
    # ä»åŸå§‹è®­ç»ƒæ—¥å¿—æˆ–configæ¢å¤å‚æ•°
    # args.pretrain_epochs = ...
    # args.max_epochs = ...
    
    if resume_epoch:
        args.resume = f"{original_checkpoint_dir}/checkpoint_epoch_{resume_epoch}.pth"
    else:
        # è‡ªåŠ¨æ‰¾åˆ°æœ€æ–°çš„checkpoint
        import glob
        checkpoints = glob.glob(f"{original_checkpoint_dir}/checkpoint_epoch_*.pth")
        latest = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
        args.resume = latest
        
    return args

# ä½¿ç”¨ç¤ºä¾‹
args = create_resume_args("checkpoints/swin_admoe_20250701_143022")
trainer_alzheimer_mmoe(args, model, "checkpoints/resumed_training")
```

### âš¡ æ€§èƒ½ä¼˜åŒ–
```python
# æ¢å¤è®­ç»ƒæ—¶çš„ä¼˜åŒ–å»ºè®®
args.eval_interval = 5     # æ›´é¢‘ç¹çš„éªŒè¯
args.save_interval = 10    # æ›´é¢‘ç¹çš„ä¿å­˜
args.patience = 15         # é€‚å½“å¢åŠ è€å¿ƒå€¼

# å¦‚æœGPUå†…å­˜ç´§å¼ 
args.batch_size = 16       # å‡å°batch size
args.accumulation_steps = 2 # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
```

---

## ğŸ‰ æ€»ç»“

| åœºæ™¯ | checkpointè·¯å¾„ | å…³é”®è®¾ç½® | é¢„æœŸè¡Œä¸º |
|------|----------------|----------|----------|
| **é¢„è®­ç»ƒä¸­æ–­** | `checkpoint_epoch_X.pth`<br/>(X < pretrain_epochs) | `args.resume = path` | ç»§ç»­SimMIMé¢„è®­ç»ƒ |
| **å¾®è°ƒä¸­æ–­** | `checkpoint_epoch_X.pth`<br/>(X >= pretrain_epochs) | `args.resume = path` | ç»§ç»­åˆ†ç±»å¾®è°ƒ |
| **æœ€ä½³æ¨¡å‹** | `best_model.pth` | `args.resume = path` | ä»æœ€ä½³ç‚¹ç»§ç»­ |

è®°ä½ï¼š**resumeä¼šå®Œæ•´æ¢å¤è®­ç»ƒçŠ¶æ€**ï¼ŒåŒ…æ‹¬epochã€optimizerã€schedulerç­‰ï¼Œç¡®ä¿è®­ç»ƒçš„è¿ç»­æ€§ï¼